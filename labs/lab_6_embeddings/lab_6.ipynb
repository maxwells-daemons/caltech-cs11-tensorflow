{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Word2Vec from scratch in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we'll write Word2Vec, a particularly popular, simple, and powerful model for unsupervised learning of embedding vectors for words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Word2Vec\n",
    "Word2Vec trains embeddings by using them as input to a logistic regression model, trained to predict a word given nearby words.\n",
    "\n",
    "Alternatively, you can think of Word2Vec as a neural network with linear activations and one hidden layer, trained to predict a word given nearby words.\n",
    "In this interpretation, the input and output of the model are both one-hot encoded vectors, and the embeddings of the words are the activations of the hidden layer given that word's one-hot encoding as input.\n",
    "Since the inputs are one-hot encoded, it's more efficient (but equivalent) to use an embedding lookup instead of a matrix multiply for the first layer.\n",
    "\n",
    "The data is produced by taking any large body of text (e.g. Wikipedia) and creating (context, target) pairs, where the target is any word and the context is the $n$ words to its left and right.\n",
    "\n",
    "There are two common ways of training Word2Vec:\n",
    " - The \"skip-gram\" model uses the target word as input and predicts context words.\n",
    " - The \"continuous bag-of-words\" (CBOW) model uses the context words as input and predicts the target word\n",
    " \n",
    "We'll focus on the CBOW model, since it tends to work better for small datasets.\n",
    "\n",
    "The process of generating the data for CBOW is as follows:\n",
    " 1. Tokenize (convert words, which are strings, to integer \"tokens\" indicating the word) the dataset by assigning each word a unique integer (integer encoding)\n",
    " 2. For each word in the dataset, create a single (`context`, `target`) pair, where `target` is the integer encoding of the word and `context` is a list of the integer encodings of the $n=1$ words to the left and right of the target word\n",
    "\n",
    "(I've already written the code to do this for you below)\n",
    " \n",
    "Then, the model is trained to predict the one-hot encoding of the target word given the integer encodings of the context words:\n",
    " 1. For each context word, look up its embedding in a table\n",
    " 2. Combine these into an \"average context embedding,\" which is the depth-wise average of the embeddings of the individual context words. This results in a single embedding vector, which acts as the \"total context\" in some sense.\n",
    " 3. Perform a logistic regression (equivalently, a single dense layer with softmax activation) to predict the target word using the average context embedding as input.\n",
    "\n",
    "Instead of full logistic regression, which uses the softmax function over all of the many words that appear in the dataset, we will use candidate sampling (specifically, noise-contrastive estimation) to compute the loss.\n",
    "This should speed up training significantly.\n",
    "\n",
    "For more info on Word2Vec, see [TensorFlow's \"Vector Representations of Words\" tutorial](https://www.tensorflow.org/tutorials/representation/word2vec), or Alex Minnaar's two tutorials, [one on the skip-gram model](http://alexminnaar.com/author/word2vec-tutorial-part-i-the-skip-gram-model.html) and [the other on the CBOW model](http://alexminnaar.com/author/word2vec-tutorial-part-ii-the-continuous-bag-of-words-model.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Download and preprocess the data\n",
    "The dataset is the Cornell Movie-Dialogs Corpus, a collection of dialogue from movie scripts.\n",
    "Download it from the link [here](http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html), then unzip it in a subfolder called \"data\".\n",
    "Your directory tree should include the file `./data/cornell movie-dialogs corpus/movie_lines.txt`.\n",
    "\n",
    "I've written all the code for loading and preprocessing the data below, but read through it to understand what's going on.\n",
    "You'll need to use parts of it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1: Read the data\n",
    "First, we read the lines as separate strings from the text file.\n",
    "\n",
    "If you are running into memory problems (i.e. running out of RAM while working on this assignment) you can increase `skip_header` to train on a smaller dataset (but your resulting embeddings will be less good).\n",
    "There are about 300,000 lines total, so you can set `skip_header` to skip some percentage of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings on incorrectly-formatted inputs\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "_, _, _, _, lines = np.genfromtxt(\n",
    "    './data/cornell movie-dialogs corpus/movie_lines.txt',\n",
    "    dtype='<U128', \n",
    "    delimiter='+++$+++', autostrip=True,\n",
    "    encoding='latin1', invalid_raise=False, unpack=True,\n",
    "    skip_header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2: Tokenize\n",
    "Keras has a built-in tokenization utility, which we'll use.\n",
    "This creates two dictionaries, which you'll need to use later:\n",
    " - `tokenizer.word_index` maps from string words to integer tokens\n",
    " - `tokenizer.index_word` maps from integer tokens to string words\n",
    "\n",
    "Then, we convert the dialogue lines into lists of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenizer and assign each word an integer\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "\n",
    "# Convert the lines to lists of integers\n",
    "tokenized_lines = tokenizer.texts_to_sequences(lines)\n",
    "\n",
    "# Delete the original lines to save memory\n",
    "del(lines)\n",
    "\n",
    "# Here's an example of how to use word_index and index_word\n",
    "print('The integer encoding of \"yes\" is:',\n",
    "      tokenizer.word_index['yes'])\n",
    "print('The word with integer encoding 10 is:',\n",
    "      tokenizer.index_word[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3: Create (context, target) pairs\n",
    "In this case, \"target\" means the integer encoding of a word, and \"context\" means a list of the integer encodings of the words to its left and right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 1  # Consider 1 word on each side of target\n",
    "stride = 2       # Avoid window overlap\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "# Add (context, target) pairs to the dataset\n",
    "for line in tokenized_lines:\n",
    "    # Do not use lines that are too short\n",
    "    if len(line) < 2 * window_size + 1:\n",
    "        continue\n",
    "    \n",
    "    for target_idx in range(window_size, \n",
    "                            len(line) - window_size, \n",
    "                            stride):\n",
    "        target = line[target_idx]\n",
    "        left_context = line[target_idx - window_size : \n",
    "                            target_idx]\n",
    "        right_context = line[target_idx + 1 :\n",
    "                             target_idx + window_size + 1]\n",
    "        \n",
    "        Y.append(target)\n",
    "        X.append(left_context + right_context)\n",
    "\n",
    "# Convert to ndarrays\n",
    "X = np.asarray(X)\n",
    "Y = np.asarray(Y)\n",
    "\n",
    "# These constants may be useful for you later\n",
    "n_words = max(tokenizer.word_index.values())\n",
    "n_examples = len(X)\n",
    "\n",
    "print('n_words:', n_words)\n",
    "print('n_examples:', n_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4: Make a TSV metadata file\n",
    "This creates a metadata file in the format that the TensorBoard Projector uses (tab-separated values).\n",
    "This will let us see the names for words later when we visualize the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Make a logs directory if none exists yet\n",
    "os.makedirs('./logs', exist_ok=True)\n",
    "\n",
    "# Make TSV metadata file\n",
    "with open('./logs/metadata.tsv', 'w') as f:\n",
    "    # Header specifies column name\n",
    "    f.write('Word\\tIndex\\tCount\\n')\n",
    "    \n",
    "    # Unrecognized values have an integer encoding of 0\n",
    "    f.write('UNK\\t0\\t0\\n')\n",
    "    \n",
    "    # One word per line\n",
    "    for i in range(1, n_words):\n",
    "        word = tokenizer.index_word[i]\n",
    "        count = tokenizer.word_counts[word]\n",
    "        f.write('{}\\t{}\\t{}\\n'.format(word, count, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5: Build a TensorFlow data pipeline\n",
    "I've set up the `tf.data.Dataset` and the iterator for you.\n",
    "Feel free to change the training hyperparameters and transforms if you like.\n",
    "\n",
    "You might want to try:\n",
    " - Changing the batch size to suit your RAM / GPU memory situation\n",
    " - Removing `.cache()` from the transforms if you can't fit the whole dataset in RAM\n",
    " - Changing n_negative_samples. Increasing it makes computing the loss slower but more accurate per batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "n_epochs = 5\n",
    "batch_size = 256\n",
    "n_batches_per_epoch = n_examples / batch_size\n",
    "print('Batches per epoch:', n_batches_per_epoch)\n",
    "\n",
    "# Number of wrong words to sample when computing\n",
    "# the loss using noise-contrastive estimation.\n",
    "n_negative_samples = 128\n",
    "\n",
    "# Construct dataset and apply transforms\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, Y))\\\n",
    "    .shuffle(1000)\\\n",
    "    .batch(batch_size)\\\n",
    "    .cache()\\\n",
    "    .repeat(n_epochs)\n",
    "\n",
    "# A one-shot iterator should be fine here\n",
    "iterator = dataset.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Build a model graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0: Model hyperparameters\n",
    "`embedding_size` is the number of dimensions in the embedding vector of each word.\n",
    "Try to train 64-dimensional embeddings, but if training takes too long, feel free to reduce this to 32 or 16.\n",
    "\n",
    "`validation_words` is a list of words we'll use to see how good our embeddings are.\n",
    "While training, we'll periodically print out the words that have embeddings most similar to these words.\n",
    "As training progresses, this should start printing words with similar meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 64\n",
    "validation_words = ['yes', 'small', 'thousand']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Input tensors\n",
    "Using the iterator, get a tensor which holds the context integer encodings (shape: `(batch_size, 2)`) and a tensor which holds the target integer encoding (shape: `(batch_size,)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Embedding variable\n",
    "Make a variable which holds the embeddings for each of the words.\n",
    "It should be a rank-2 tensor (a matrix) where the $i$-th row is the embedding vector for the word with integer encoding $i$.\n",
    "Its shape should be `(n_words, embedding_size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Compute the average context embedding\n",
    "Look up the embeddings of each of the context words (with a single call to `tf.nn.embedding_lookup`), then average them depth-wise into a single average embedding vector with shape `(batch_size, embedding_size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4: Create the variables for logistic regression\n",
    "Create weight and bias variables for a logistic regression which takes in the average context embedding and outputs a probability for each word.\n",
    "\n",
    "NOTE: We won't actually ever compute the output of this logistic regression by hand (i.e. the matrix multiplication and softmax activation), since we're using noise-contrastive estimation to approximate it.\n",
    "TensorFlow's `nce_loss` function just takes the weights and bias tensors as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5: Compute the loss\n",
    "We want to jointly train the logistic regression weights and the word embeddings using cross-entropy loss on the output of the logistic regression, but this is inefficient because of the large number of outputs (we'd need one logit for each word).\n",
    "\n",
    "Instead, approximate the per-example loss using noise-contrastive estimation by calling `tf.nn.nce_loss`, then compute the mean loss for the batch, and add a summary scalar to plot the loss in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7: Optimizer and gradients\n",
    "Make an optimizer (I used `tf.train.AdamOptimizer` with `learning_rate=1e-3`) and an operation to apply the gradient updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8: Operations find similar embeddings\n",
    "The **cosine similarity** between two embeddings $\\vec{x}$ and $\\vec{y}$ is the cosine of the angle between them, computed as:\n",
    "$$\n",
    "\\cos(\\vec{x}, \\vec{y}) = \\frac{\\vec{x} \\cdot \\vec{y}}{||\\vec{x}|| \\cdot ||\\vec{y}||}\n",
    "$$\n",
    "This is a more robust similarity measure than Euclidean distance, since it's invariant to scaling the embedding vectors by a constant.\n",
    "\n",
    "We'll use cosine similarity to find the most similar words to any given input word.\n",
    "The steps for doing this are:\n",
    " 1. Take a word's integer encoding as input, and compute its embedding (I've done this for you)\n",
    " 2. Compute a \"similarity tensor\" which holds cosine similarity of this embedding with all of the word embeddings. This should result in a single tensor with shape `(n_words,).\n",
    " 3. Use `tf.nn.top_k` to find the top 8 similarities and their indices in the similarity tensor. These indices will be the integer encodings of the words with the most similar embeddings to the input.\n",
    "\n",
    "Later, you can find the most similar words to a given input word by running (in a session) the tensor that holds the top 8 indices, then using `tokenizer.index_word` on each of those indices.\n",
    "\n",
    "When debugging this, make sure that the most similar word to a given word is that word itself, with a cosine similarity of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('compute_similar_words'):\n",
    "    input_word = tf.placeholder(tf.int64, shape=(), name='input_word')\n",
    "    input_embedding = tf.nn.embedding_lookup(word_embeddings,\n",
    "                                             input_word,\n",
    "                                             name='input_embedding')\n",
    "    \n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Train the model\n",
    "Use the same kind of training loop we've used before, repeatedly calling the operation that applies gradient updates.\n",
    "\n",
    "The dataset is large, but the model doesn't do much work for each example.\n",
    "So, you might want to only run and save the summaries every 1000 batches or so to speed up training.\n",
    "\n",
    "In addition, every 1000 batches, for every word in `validation_words`, print the 8 most similar words by cosine similarity.\n",
    "\n",
    "Finally, use a `tf.train.Saver()` to save the model in `./logs`, which is necessary to visualize the embeddings in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Visualize the learned embeddings\n",
    "Run TensorBoard pointed at `./logs` and look in the Projector tab, then use the \"Load data\" button and select the `./logs/metadata.tsv` file we created earlier to load word labels.\n",
    "\n",
    "Try typing some words into the search bar and see which words come up as most similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 (optional): Generate analogies\n",
    "Try using embedding vector arithmetic to generate analogies.\n",
    "To do this:\n",
    " - Load the saved model into a `tf.Session()` with the saver\n",
    " - Compute the embedding vector for several vectors using `feed_dict` and `tf.nn.embedding_lookup`\n",
    " - Do vector arithmetic on the computed embeddings\n",
    " - Find the most similar word embeddings by cosine similarity, then find the words that map to those embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
