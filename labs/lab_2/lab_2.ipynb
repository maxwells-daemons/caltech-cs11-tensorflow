{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Linear Models\n",
    "This lab, like the others that follow it, will be a small, largely self-guided project in building a machine learning model.\n",
    "You will write a logistic regression classifier, from scratch, to classify 28x28 pixel images of handwritten digits (0 - 9) by which digit appears in the image.\n",
    "This is [the famous MNIST dataset](http://yann.lecun.com/exdb/mnist/), which has 60,000 training examples and 10,000 test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "# Any other imports you need go here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: download the data\n",
    "Download the data as a CSV [here](https://pjreddie.com/projects/mnist-in-csv/) (the original data format is very unfriendly) and read the data format on the website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: load, understand, and preprocess the data\n",
    "Load the data from disk into four numpy arrays: x_train (feature vectors from the training set), y_train (labels from the training set), x_test (feature vectors from the test set), and y_test (labels from the test set).\n",
    "\n",
    "Then, do some simple preprocessing.\n",
    "Normalize your features by subtracting the mean and dividing by the range.\n",
    "This is less important for linear models, but is generally good practice and will be much more important with more complex models.\n",
    "Make sure you don't use any values computed from the test set, just the mean and range of the training set.\n",
    "\n",
    "Finally, display one of the training images, and print its correct label.\n",
    "\n",
    "Functions to look at:\n",
    " - `np.loadtxt`\n",
    " - `np.reshape`\n",
    " - `plt.imshow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: build a data pipeline\n",
    "Make a pipeline that turns the dataset you have in numpy arrays into tensors that your model can use.\n",
    "The pipeline should also shuffle and batch the data (with some reasonable batch size; I used 64 because I'm training on a GPU but you might want to go lower).\n",
    "\n",
    "There are multiple ways to approach this.\n",
    "No matter what, you'll need two `tf.data.Dataset`s, one for train and one for test, and probably you'll do some transforms to that.\n",
    "I made datasets which did not use `repeat`, and used reinitializable iterators to feed the data -- you might instead repeat the data for as many epochs (full run-throughs of the dataset during training, ~5-10 for this assignment) as you plan to train for, then use one-shot iterators instead, or do something else.\n",
    "Don't use feeding to pass in your data, though, since it'll result in a slow pipeline.\n",
    "\n",
    "Optionally, you might want to cache or prefetch data to prevent it from being loaded multiple times while the model is training and keep the model from needing to wait for data.\n",
    "\n",
    "Now might also be a good place to convert the labels to one-hot encoding (see below), though you don't need to.\n",
    "If you want, you could also do your data normalization on-the-fly here (but make sure it still does the same thing).\n",
    "\n",
    "Functions to look at (not exhaustive):\n",
    " - `tf.data.Dataset.from_tensor_slices`\n",
    " - `tf.data.Dataset.shuffle`\n",
    " - `tf.data.Dataset.batch`\n",
    " - `tf.data.Dataset.repeat`\n",
    " - `tf.data.Dataset.make_one_shot_iterator`\n",
    " - `tf.data.Dataset.make_initializable_iterator`\n",
    " - `tf.data.Iterator.from_structure`\n",
    " - `tf.data.Iterator.make_initializer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: build a model graph\n",
    "This is where the actual model gets built.\n",
    "I'll give you the steps, but there's flexibility in how you implement each part.\n",
    "Make sure your design is clean, both for efficiency and ease of debugging.\n",
    "Names and name scopes help divide the model into logical parts.\n",
    "\n",
    "Note that the first axis of every tensor will be the batch size.\n",
    "So, a tensor that in your model is a scalar will likely have a shape of (?), and a 10-element vector will have a shape of (?, 10).\n",
    "The \"?\" indicates that TensorFlow will treat this axis as being variable-length, since it can't infer it just from context.\n",
    "It may be helpful to switch your thinking back and forth from one view (no batches, scalars are scalars) when thinking about modeling to another (scalars come in batches) when writing code that changes shapes.\n",
    "\n",
    "If you're having difficulty debugging, try looking at your graph in TensorBoard, printing tensor objects to see their shapes, and running small parts of the graph in a session while feeding values to certain tensors.\n",
    "\n",
    "Functions to look at throughout:\n",
    " - `tf.cast`\n",
    " - `tf.expand_dims`\n",
    " - `tf.squeeze`\n",
    " - arithmetic operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Input tensors\n",
    "Get a tensor for the input image (features) and another tensor for the correct label.\n",
    "It may be convenient to convert the label to a one-hot vector: 10 elements, each of which is 0 except the place of the correct label.\n",
    "For instance, a label of \"3\" would be the vector $$[0, 0, 0, 1, 0, 0, 0, 0, 0, 0].$$\n",
    "\n",
    "Functions to look at:\n",
    " - `tf.data.Iterator.get_next`\n",
    " - `tf.one_hot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Variables\n",
    "Logistic regression needs two variables, correctly shaped: a weight matrix and a vector of biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Logits\n",
    "Compute the inputs to the softmax functiton, the logits or \"unnormalized probabilities.\"\n",
    "The result should be, for each example in the batch, a vector of ten values (so the tensor has a shape like (?, 10))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4: Softmax probabilities\n",
    "Apply the softmax function to the logits to obtain a vector of class probabilities.\n",
    "Again, there should be a probability for each class for each example.\n",
    "Do _not_ use helper functions to compute the softmax function, like `tf.nn.softmax` or `tf.contrib.layers.softmax`.\n",
    "Instead, stick to core operations like `tf.exp`.\n",
    "\n",
    "Two main reasons for this:\n",
    " 1. Better to learn without the API doing too much of the work for you; when doing model development it'll basically all be from scratch\n",
    " 2. I couldn't get `tf.nn.softmax` to run on a GPU, so I'd do it this way in practice too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5: Cross-entropy loss\n",
    "Compute the per-example cross-entropy loss $$L = -y \\cdot \\log p_\\text{model}(y)$$ using the probabilities and the correct label.\n",
    "\n",
    "Then, take the mean of the per-example losses to compute a per-batch loss.\n",
    "Also, add a summary operation which saves the per-batch loss so you can plot it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6: The optimizer\n",
    "Add an optimizer (simple gradient descent is fine) and an operation to minimize the per-batch loss.\n",
    "You may need to play around with the learning rate to find one that works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7: Predicted label and per-batch accuracy\n",
    "The model should predict the digit it assigns the highest probability. \n",
    "Add a tensor which represents the predicted digit; this lets you use the model \"in the wild\".\n",
    "Then, add a tensor which represents what fraction of the batch the model predicted correctly (its accuracy, or average 0/1 loss), and a summary operation for accuracy.\n",
    "\n",
    "Finally, add a tensor to merge the summaries made so far.\n",
    "Or, just add them one at a time if you want (but why would you?)\n",
    "\n",
    "Functions to look at:\n",
    " - `tf.argmax`\n",
    " - `tf.equal`\n",
    " - `tf.reduce_mean`\n",
    " - `tf.summary.merge_all`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: train the model\n",
    "Make a training loop which iterates through the full training set multiple times, and, for each batch:\n",
    " 1. Runs the training operation\n",
    " 2. Adds the summaries to a training FileWriter, with the correct batch number\n",
    "\n",
    "After each epoch, iterate over the test dataset, adding summaries to a test FileWriter, and print the average test accuracy over the whole test set.\n",
    "Finally, use a `Saver` to save the whole graph to disk.\n",
    "\n",
    "Remember to: \n",
    " - Initialize variables\n",
    " - Give FileWriters references to the graph\n",
    " \n",
    "Then, run training.\n",
    "You should expect the model to hit about 90% test-set accuracy.\n",
    "Not bad for a linear model!\n",
    "If it doesn't, it might indicate a bug in your code, or you might need to tune hyperparameters (e.g. batch size, learning rate).\n",
    "The accuracy should sharply jump after the first epoch, so you don't need to wait for the full model to train to know you have a bug. \n",
    "\n",
    "You might notice, looking at the training plots on TensorBoard, how close the training and test errors are.\n",
    "This suggests our model is probably underfitting, and a more powerful model will do better.\n",
    "\n",
    "<sup><sub>My model was showing 20% accuracy and it took over an hour to figure out I was actually just computing accuracy wrong :(</sub></sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: use the model for inference\n",
    "Pull one or more examples from the test set and display its image.\n",
    "Then, use your model to predict what digit the image is, print that, and print the true label (which hopefully matches).\n",
    "\n",
    "Remember you'll need to use `Saver.restore()` to load the trained model before inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: visualizations\n",
    "Visualize, as an image (i.e. `plt.imshow`), the weight matrix for each of the digits.\n",
    "You might want to use the `vmin` and `vmax` arguments to put all of the matrices on the same scale. \n",
    "You'll need to load the weights again to save them to numpy arrays (by `run()`-ing the variables).\n",
    "\n",
    "The results are pretty interesting.\n",
    "The image should be brightest where the weights strongly indicate that digit, so you can see what's (linearly) characteristic of different digits -- I see a strong dark spot in the middle for zero, a distinctive curly tail on 2, and a consistent 3.\n",
    "Other digits (e.g. 8 and 9) have much weaker patterns, suggesting greater variation in how people draw them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Keras\n",
    "Finally, make the same model with Keras and train it.\n",
    "You should again see about 90% test-set accuracy.\n",
    "\n",
    "Hint: logistic regression is just linear regression followed by the softmax function.\n",
    "\n",
    "Functions to look at:\n",
    " - `keras.utils.to_categorical` (for making one-hot labels)\n",
    " - `keras.layers.Dense`\n",
    " - `keras.layers.Activation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
