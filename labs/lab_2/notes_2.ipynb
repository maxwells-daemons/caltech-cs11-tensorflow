{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning: classification and regression\n",
    "For now, we'll stick to supervised learning: the case of machine learning where we want to learn a function from input features $\\vec{x}$ to some output $y$, for which we have a dataset of many examples of inputs and their associated \"true\" outputs $\\{\\vec{x}_i, y_i\\}$.\n",
    "\n",
    "Simple supervised learning problems are split into two cases.\n",
    "**Regression** is the case where each $y_i$ is continuous, a real-valued number, and **classification** is the case where $y_i$ is discrete, one of a few classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions\n",
    "The loss function maps from the parameters $\\theta$ of a model, and the set of examples (features and their true labels, $\\{\\vec{x_i}, y_i\\}_i$), to a single value that represents how badly the model fits the data.\n",
    "Minimizing the loss function is equivalent to finding the \"optimum\" parameter settings, in the sense that if our model was generating the data, it is the model with these parameters that is most likely to have generated the data (instead of a model structured the same way with any other parameters).\n",
    "\n",
    "We can justify this kind of inversion using Bayes' theorem:\n",
    "$$\n",
    "p(\\vec{\\theta} | \\text{data}) = \\frac{p(\\text{data} | \\vec{\\theta}) \\cdot p(\\vec{\\theta})}{p(\\text{data})} \\propto p(\\text{data} | \\vec{\\theta})\n",
    "$$\n",
    "Therefore maximizing the probability that our model was the one that generated the data is equivalent to finding the most probable parameters given the data, which is exactly what we want to do.\n",
    "We can also compute the loss value on held-out values (a validation set) to see how well the model generalizes to unseen data.\n",
    "\n",
    "To make learning feasible, we usually assume that the loss function is possible to express as an average of per-example loss functions:\n",
    "$$ L_\\text{total}(\\{\\vec{x_i}, y_i\\}_i^n, \\vec{\\theta}) = \\frac{1}{n} \\sum_i^n L((\\vec{x}_i, y_i), \\vec{\\theta}) $$\n",
    "\n",
    "Often the loss over the entire dataset is called a \"cost function\" and the loss over individual examples is just called a \"loss function,\" so I'll use that notation from now on.\n",
    "Either way, it's important that the loss function is differentiable so we can minimize it with gradient descent.\n",
    "\n",
    "### Squared error\n",
    "The (mean) squared error (MSE) is the \"default\" loss function for regression:\n",
    "$$ L(y, \\hat{y}) = (y - \\hat{y})^2 $$\n",
    "where $y$ is the true value to be predicted, and $\\hat{y}$ is the model's prediction.\n",
    "\n",
    "It penalizes the model for mistakes super-linearly, giving larger weight to big mistakes than small ones.\n",
    "This can make MSE vulnerable to outliers.\n",
    "\n",
    "### Cross-entropy\n",
    "Cross-entropy loss (or \"log loss\") is the most common loss function for classification.\n",
    "\n",
    "The output of a classification model is a vector of probabilities, one per class, that should sum up to one.\n",
    "The $i$th probability represents how confident the model is that the input corresponds to class $i$.\n",
    "In an information-theoretic sense, cross-entropy loss measures how different the probability distribution given by the model is from the empirically-measured distribution for that one example (0 probability for every class except the true one, which has probability 1).\n",
    "\n",
    "As the model's confidence in the correct class approaches 1, the cross-entropy loss approaches 0.\n",
    "As the model's confidence in the correct class approaches 0, the cross-entropy loss increases to infinity.\n",
    "\n",
    "#### The binary case\n",
    "In binary classification, your model outputs a single probability $p(x)$: the probability that the output label is 1 (which is one minus the probability that the label is 0).\n",
    "The true label $y$ is 0 or 1, and the loss is\n",
    "$$L(\\vec{x}, y; \\vec{\\theta}) = - y \\cdot \\log (p) - (1 - y) \\cdot \\log (1 - p)\n",
    "= \\begin{cases} -\\log(p), & \\text{if } y = 0 \\\\ -\\log(1-p), & \\text{if } y = 1 \\end{cases}\n",
    "$$\n",
    "\n",
    "#### The multi-class case\n",
    "In multi-class classification (with multinomial cross-entropy loss), we turn the true label into a \"one-hot encoding\" -- a vector with one entry per possible class, that's zero everywhere except the true class.\n",
    "For example, for a problem with $n=4$ classes and an example with correct label of 1, the one-hot encoding is \n",
    "$$p_\\text{true}(y) = \\begin{bmatrix}0 & 1 & 0 & 0 \\end{bmatrix}$$\n",
    "which defines a discrete \"probability distribution\" over possible labels that just assigns probability 1 to the correct label.\n",
    "Our model outputs a vector of confidences, $p_\\text{model}(y)$.\n",
    "The loss is\n",
    "$$L = -\\sum_{i=0}^n p_\\text{true}(y_i) \\log (p_\\text{model}(y_i))\n",
    "= \\begin{cases} - \\log p_\\text{model}(y_i) & \\text{if } y = i \\\\ \\vdots \\end{cases}$$\n",
    "Often this is computed by taking the dot product of the one-hot encoded label vector with the probability vector, then taking the log.\n",
    "\n",
    "Note that this loss function only cares about what probability the model assigns to the correct class, not any of the other classes.\n",
    "This can make computing cross-entropy loss more efficient (i.e. by not computing the probabilities assigned to any other class).\n",
    "\n",
    "#### Aside: the other terms in the Bayes theorem derivation\n",
    "There are two terms we didn't talk about.\n",
    "$p(\\text{data})$ is the _probability of seeing the data we did under the true data generatng process_, commonly called the \"evidence\", and it generally doesn't impact our model-building. \n",
    "\n",
    "The other term, $p(\\theta)$ is much more interesting. \n",
    "It's our prior belief, represented as a probability distribution, about what kinds of parameters we expect to see.\n",
    "In machine learning, we almost always express our priors through how the model is structured, how we preprocess (or augment) the data, and what kind of regularization we use.\n",
    "Using neural networks (next week), for instance, expresses a strong prior that the function we want to learn is a _hierarchical composition of simple patterns_.\n",
    "\n",
    "#### Aside: other loss functions\n",
    "Lots of the time, other loss functions make sense to use.\n",
    "It's problem-dependent.\n",
    "For example, you might want to use dice-coefficient loss to deal with unbalanced classes in problems like classifying each pixel of an image (the \"semantic segmentation\" problem).\n",
    "MSE and cross-entropy are good default choices, though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic and minibatch gradient descent\n",
    "Since the loss over the entire dataset decomposes into an average of per-example losses, we can approximate the total loss with an average over a smaller number of examples:\n",
    "$$L(\\{\\vec{x}_i, y_i\\}; \\vec{\\theta}) = \\frac{1}{n} \\sum_i^n L((\\vec{x}_i, y_i), \\vec{\\theta}) \\approx \\frac{1}{m} \\sum_i^m L((\\vec{x}_i, y_i), \\vec{\\theta})$$\n",
    "where $m$ is the \"batch size\", or the number of examples used to approximate the true dataset loss in this case.\n",
    "\n",
    "This is called \"minibatch gradient descent\" or \"stochastic gradient descent\" (SGD) (sometimes this term is restricted for $m=1$).\n",
    "It can converge faster than full-scale gradient descent because often small batches are sufficient to approximate the average loss reasonably well, and the average direction of steps taken in the direction opposite the gradient of the average minibatch loss is the same as if you were using the dataset loss.\n",
    "As a result, minibatch gradient descent is considered the best way to train most machine learning models.\n",
    "\n",
    "Some considerations when picking a batch size:\n",
    " - Smaller batches provide a less-accurate estimate of the direction to move parameters in at each step\n",
    " - Larger batches take a longer time to compute\n",
    " - [There is evidence that the noise introduced from small batches is beneficial for reaching optima that generalize out-of-sample (geometrically, \"broad\" minima instead of \"sharp\" ones)](https://arxiv.org/abs/1609.04836)\n",
    " - [In some sense lowering the learning rate and increasing the batch size have the same effect](https://arxiv.org/abs/1711.00489)\n",
    " - Large batch sizes compute faster per-example on GPUs, making training the model overall faster (by reducing the frequency with which the GPU and CPU need to swap memory)\n",
    " - Large batches may not fit on the available memory (eg video RAM in a GPU)\n",
    " - Power-of-two batch sizes compute faster on GPUs (because of how \"tensor cores\" work)\n",
    " \n",
    "Common batch sizes are anywhere from 4 to 64, sometimes higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "Linear regression is the simplest model we'll cover.\n",
    "Linear regression of one variable involves as parameters a vector of weights $\\vec{w}$ and a scalar bias $b$; given a vector of input features $\\vec{x}$, the model predicts $\\hat{y} = \\vec{w}^T \\vec{x} + b$.\n",
    "The output variable is a linear combination of the inputs weighted by the weight vector, plus a bias that captures something like the mean output.\n",
    "\n",
    "While linear algebra gives us a closed-form solution, in practice it's ineffient with large datasets.\n",
    "Instead, learning $\\vec{w}$ and $b$ with gradient descent on mean-squared error produces the same optimum, since the learning problem for linear regression is convex.\n",
    "\n",
    "Linear regression can actually be used to regress against any handmade set of features computed from the data, so long as the output is expected to be a linear combination of the features.\n",
    "For instance, we can fit a quadratic polynomial by computing the square of every input feature, and treating them as completely new features: \n",
    "$$\\vec{x} = \\begin{bmatrix} x_1 & x_2 & \\cdots \\end{bmatrix} \\rightarrow  \\begin{bmatrix} x_1 & x_2 & \\cdots & x_1^2 & x_2^2 & \\cdots \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "Despite the name, logistic regression is the linear model for classification.\n",
    "Linear regression on the probability that a particular class is right doesn't make much sense, because:\n",
    " - Individual class probabilities must be in the range [0, 1] and linear regression may produce values outside of this range\n",
    " - All class probabilities must sum to 1\n",
    "\n",
    "Logistic regression models follow a few steps:\n",
    " 1. For each class, perform linear regression to an \"unnormalized probability\" for that class. This value is called the **logit**, and it corresponds to the natural log of the odds (as opposed to probability) that the class is correct. This linear regression makes sense, since the log-odds can take any value from $-\\infty$ to $\\infty$.\n",
    " 2. Convert the vector of log-odds to a valid probability distribution. In the one-variable case, this is done through the _logistic function_. In the multi-variable case, it means applying the _softmax function_.\n",
    " 3. Train the model (usually using cross-entropy loss) on the computed probability distribution over classes.\n",
    "\n",
    "#### One-variable case and the logistic function\n",
    "In the one-variable case, $y \\in \\{0, 1\\}$ and the model is as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&l = \\vec{\\alpha}^T \\vec{x} + b & \\text{where $l$ is the logit, the log of the odds that $y=1$} \\\\\n",
    "&p = \\sigma(l) = \\frac{e^l}{e^l + 1} & \\text{where $\\sigma$ is the logistic function and $p$ is the model probability that $y = 1$} \\\\ \n",
    "&L = -y \\log(p) - (1 - y) \\log (1 - p) & \\text{where $L$ is the (cross-entropy) loss}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The logistic function converts from log-odds to probability by exponentiating the logit (to undo the log), then computing the ratio of the odds (numerator) to the total odds (denominator).\n",
    "![logistic function](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg)\n",
    "Image credit: [Wikipedia article on logistic regression](https://en.wikipedia.org/wiki/Logistic_regression)\n",
    "\n",
    "The logistic function...\n",
    " - Maps every input to the range \\[0, 1\\]\n",
    " - Is continuously increasing\n",
    " - \"Saturates\" (getting near-zero derivative) for high and low input values\n",
    " \n",
    "#### Multivariable case and the softmax function\n",
    "This can be generalized to the case of more than one variable.\n",
    "In this case, there is one logit value per class, $l_i$, and so there is one linear regression per class.\n",
    "Then the softmax function is applied, which converts the logits into probabilities:\n",
    "$$ p_i = \\frac{\\exp(l_i)}{\\sum_j \\exp(l_j)}$$\n",
    "It works exactly the same way as the logistic function, taking the ratio of the unnormalized probability of one class to the total of unnormalized probabilities.\n",
    "\n",
    "The probabilities computed by the softmax function...\n",
    " - Are each individually in the range \\[0, 1\\]\n",
    " - Together sum to 1\n",
    " - Increase monotonically with their associated logit\n",
    " \n",
    "Another interpretation of the softmax function is that it's a \"softened\" or \"smoothed\" version of the argmax function, which sets the largest element of a vector to 1 and all other elements to 0.\n",
    "The softmax function just makes the largest element near 1 and all of the other elements near 0 -- the fact that it's \"softened\" allows it to be differentiable, and so we can use it for gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation and the chain rule\n",
    "The backpropagation algorithm, and how it combines with gradient descent to make differentiable programming a powerful way of writing models, is probably the most important concept from this class.\n",
    "\n",
    "The critical idea is this: _if you have a computational graph made of only differentiable operations, you can efficiently compute the derivative of any one value with respect to every other value at once_.\n",
    "\n",
    "By using the [multivariate chain rule](https://en.wikipedia.org/wiki/Chain_rule#Higher_dimensions), we can easily (and automatically!) compute the derivative of a value with respect to a single other value, assuming the derivatives of each individual operation are known:\n",
    "![backpropagation on a computational graph](https://colah.github.io/posts/2015-08-Backprop/img/tree-eval-derivs.png)\n",
    "In the above graph, all of the operations, values, and single-edge derivatives are labeled.\n",
    "To compute the partial derivative of one value $A$ with respect to another value $B$, traverse every possible path from $B$ to $A$; within a path, multiply each of the edge partial derivatives together, and at the end add up all of the path derivatives to get the _total derivative_.\n",
    "For instance, $\\frac{\\partial e}{\\partial b} = (\\frac{\\partial e}{\\partial c} \\times \\frac{\\partial c}{\\partial b}) + (\\frac{\\partial e}{\\partial d} \\times \\frac{\\partial d}{\\partial b}) = (1 \\times 2) + (1 \\times 3) = 5$.\n",
    "\n",
    "To understand this, **convince yourself** (really!) that:\n",
    " 1. The single-variable chain rule means that multiplying the partial derivatives along a single path to get the contribution of that path to the derivative of the output\n",
    " 2. The multi-variable chain rule means that we can add each of the paths from $b$ to $e$ to get its total contribution to the derivative\n",
    "\n",
    "This process of **automatic differentiation** (abusing terminology slightly) makes it possible to compute the derivative of a single value with respect to another single value by traversing each path once.\n",
    "\n",
    "However, if we want to compute the gradient of the loss function over a minibatch with respect to the parameters of our model (i.e. the `tf.Variable`s) so we can update them all with gradient descent, we need the partial derivative of the loss with respect to each parameter individually.\n",
    "Naively \"summing over paths\" means we might wind up computing the same value many times, making the algorithm inefficient.\n",
    "\n",
    "Instead, we step backwards through the graph, first computing the partial derivative with respect to the direct parents of the desired node ($\\frac{\\partial e}{\\partial c}$ and $\\frac{\\partial e}{\\partial d}$), then computing the partial derivative with respect to their parents, and so on.\n",
    "You will notice that each of these computations makes use of values we already have (namely, the partial derivatives computed further-along in the graph) and only a single new edge, multiplying the saved derivative by the newly-computed derivative on that edge.\n",
    "Thus, once we've computed the partial derivative of the loss with respect to every parameter, we have computed the derivative on each edge exactly once.\n",
    "\n",
    "You should think of _values flowing forwards through a network_ (and being saved, since they're needed for differentiation) and _derivatives flowing backwards_.\n",
    "Alternatively, this is a dynamic-programming solution to the problem of computing all of these derivatives, where we get a (worst-case) exponential-time speedup by computing the values in the correct order and caching them.\n",
    "Computing the gradient of a single value (the loss function) with respect to every value in the graph takes only time time to compute a _forwards pass_ (to compute the loss), plus the sum of the times it takes to compute the derivatives of every individual operation (the _backwards pass_).\n",
    "\n",
    "The end result is a beautiful system where so long as we can build a model out of differentiable operations, we can efficiently optimize it with gradient descent (subject, of course, to optimization's usual quibbles about smoothness assumptions, local minima, etc).\n",
    "This is what differentiable programming is all about!\n",
    "\n",
    "#### Outside reading on backpropagation\n",
    "Since this is a three unit class I can't require you to read these, but I can _promise_ they'll be worth your time, and that once you read all three you'll deeply understand one of the most important algorithms in the modern world:\n",
    " - [\"Calculus on Computational Graphs: Backpropagation\" (Chris Olah)](https://colah.github.io/posts/2015-08-Backprop/) quickly and intuitively derives backpropagation. If you only read one of these resources, make it this one.\n",
    " - [\"Hacker's guide to Neural Networks\" (Andrej Karpathy)](https://karpathy.github.io/neuralnets/) derives the chain rule and backpropagation from scratch, from the perspective of math as combining \"circuits\" (operations) and from a \"hacking\" or \"coding\" perspective\n",
    " \n",
    "[Image credit: Colah's Blog](https://colah.github.io/posts/2015-08-Backprop/)\n",
    "(Also, credit to the blog for examples I've taken here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "In this case, I can't give a better explanation than [the TensorFlow official guide](https://www.tensorflow.org/guide/datasets).\n",
    "Read the following sections:\n",
    " - [Basic mechanics (all subsections)](https://www.tensorflow.org/guide/datasets#basic_mechanics)\n",
    " - [Consuming NumPy arrays](https://www.tensorflow.org/guide/datasets#consuming_numpy_arrays)\n",
    " - [Batching dataset elements](https://www.tensorflow.org/guide/datasets#batching_dataset_elements)\n",
    " - [Training workflows](https://www.tensorflow.org/guide/datasets#training_workflows)\n",
    " \n",
    "For this class, don't use feeding to input data ever (it's slow), and don't use `tf.train.MonitoredTrainingSession` (it's meant for distributing training over many machines).\n",
    "\n",
    "I prefer to use reinitializable iterators so I can do end-of-epoch processing and treat multiple datasets (e.g. training and validation) transparently with the same operations.\n",
    "But, how you structure your input pipeline is mostly up to you (and what makes sense for the problem).\n",
    "\n",
    "If you're interested in making performant data pipelines, check out the [official guide on data input pipeline performance](https://www.tensorflow.org/guide/performance/datasets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and restoring variables\n",
    "When starting a new session, the values of variables are lost -- they must be initialized and trained again, or loaded from a file.\n",
    "To save all of the variables in a graph to a file, first create a `tf.train.Saver()`, then run `saver.save(sess, 'file_path')` where `sess` is a `tf.Session`.\n",
    "To restore the variables again, run `saver.restore(sess, 'file_path')`.\n",
    "\n",
    "Note that in this case, 'file_path' is actually a prefix to a number of checkpoint files.\n",
    "\n",
    "To learn more (saving and loading only specific variables, and the `SavedModel` abstraction for serving models), look at the [official guide](https://www.tensorflow.org/guide/saved_model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: linear regression (TensorFlow)\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
