{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Reading Sign Language with Convoltuional Networks\n",
    "CNNs have revolutionalized basically every problem that takes an image as input, and the simplest of these is image classification.\n",
    "\n",
    "This project involves using TensorFlow and Keras to build a program that recognizes numbers 0-9 in sign language by classifying images of hands signing those digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Download the data\n",
    "The data is available on Kaggle, at https://www.kaggle.com/ardamavi/sign-language-digits-dataset/.\n",
    "You'll need an account to download it; let me know if you can't do this.\n",
    "\n",
    "Make a directory called `data`, then unzip the data files inside that directory.\n",
    "Your final directory structure should contain files:\n",
    " - `.../lab_4_cnn/data/X.npy`\n",
    " - `.../lab_4_cnn/data/Y.npy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Understand the data\n",
    "I've taken care of loading the data for you.\n",
    "Read through the code (especially comments) so you understand what it does, and check out the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = np.load('data/X.npy')\n",
    "y_all = np.load('data/Y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For whatever reason, the data's labels aren't the actual\n",
    "# numbers depicted. This box fixes that.\n",
    "# Real-world data is usually messy; this is one example.\n",
    "\n",
    "# Maps dataset-provided label to true label\n",
    "label_map = {0:9, 1:0, 2:7, 3:6, 4:1, 5:8, 6:4, 7:3, 8:2, 9:5}\n",
    "\n",
    "# Correct dataset labels\n",
    "for row in range(y_all.shape[0]):\n",
    "    dataset_label = np.where(y_all[row])[0][0]\n",
    "    y_all[row, :] = np.zeros(10)\n",
    "    y_all[row, label_map[dataset_label]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed numpy rng for reproducibility\n",
    "np.random.seed(1337)\n",
    "\n",
    "# Shuffle features and targets together\n",
    "# Credit for this technique to:\n",
    "# https://stackoverflow.com/questions/4601373/\n",
    "# better-way-to-shuffle-two-numpy-arrays-in-unison\n",
    "rng_state = np.random.get_state()\n",
    "np.random.shuffle(x_all)\n",
    "np.random.set_state(rng_state)\n",
    "np.random.shuffle(y_all)\n",
    "\n",
    "# Add a dummy channel axis to input images\n",
    "x_all = np.expand_dims(x_all, axis=-1)\n",
    "\n",
    "# Center and rescale data to the range [-1, 1]\n",
    "x_all = x_all - 0.5\n",
    "x_all = x_all * 2\n",
    "\n",
    "# Create a validation set from 30% of the available data\n",
    "n_points = x_all.shape[0]\n",
    "n_test = int(n_points * 0.3)\n",
    "n_train = n_points - n_test\n",
    "x_train, x_test = np.split(x_all, [n_train], axis=0)\n",
    "y_train, y_test = np.split(y_all, [n_train], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print important shapes in the dataset\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the first example of each digit in the training set.\n",
    "### You don't need to understand the code, just look at the output.\n",
    "\n",
    "# Set up plots\n",
    "plots = plt.subplots(nrows=2, ncols=5, figsize=(10, 4))\n",
    "axes_list = plots[1].ravel()\n",
    "\n",
    "for digit in range(10):\n",
    "    axes = axes_list[digit]\n",
    "    axes.set_axis_off()\n",
    "    axes.set_title(digit)\n",
    "  \n",
    "    # Find the index of the first appearance of this digit\n",
    "    idx = np.where(y_train[:, digit] == 1)[0][0]\n",
    "    \n",
    "    # Plot the image\n",
    "    axes.imshow(x_train[idx, :, :, 0],\n",
    "                cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Build a TensorFlow data pipeline\n",
    "Set up any `tf.data.Dataset` and `tf.data.Iterator` objects you need.\n",
    "\n",
    "I used two `Dataset`s and a single reinitializable `Iterator`, but there are multiple ways to solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Build a model graph\n",
    "We'll be building a fairly \"traditional\" image-processsing CNN: a few layers of 2-D convolutions and max pooling, then flattening, dense layers, and an output layer.\n",
    "\n",
    "Feel free to experiment with the model architecture.\n",
    "With a simple network, expect around 90% accuracy.\n",
    "(Logistic regression gets about 75%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Input tensors\n",
    "Get a tensor for the input image and another for the correct label.\n",
    "Note that the label is already one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Helper function to make dense layers\n",
    "Copy the `make_dense_layer()` function you wrote last week here, since this model will need dense layers too.\n",
    "\n",
    "(In practice, we'd use `tf.layers.Dense`, which does basically the same thing, but using your code from last week gives you more flexibility to do things like plot histograms with minimal extra work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dense_layer(prev_activations, dim_input, dim_output, \n",
    "                     do_activation=True, postfix=''):\n",
    "    '''\n",
    "    Adds a dense layer to the model graph.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prev_activations: tensor\n",
    "        The activations of the previous layer, or \n",
    "        the input for the first dense layer.\n",
    "    dim_input: int\n",
    "        Number of features in the input representation.\n",
    "    dim_output: int\n",
    "        Number of features in the output representation.\n",
    "        Equivalently, number of units in this layer.\n",
    "    do_activation: bool\n",
    "        Whether or not to apply ReLU activation.\n",
    "    postfix: string\n",
    "        Postfix on name and variable scopes in this layer.\n",
    "        Used to simplify visualizations.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A tensor representing the activations of this layer.\n",
    "    '''\n",
    "    with tf.name_scope('dense' + postfix):\n",
    "        with tf.variable_scope('dense' + postfix):\n",
    "            # Define variables here\n",
    "        # Define operations here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Helper function to make convolutional layers\n",
    "Write a function, similar to `make_dense_layer()`, called `make_conv_layer()`, which has its signatures and scopes defined as a stub below.\n",
    "When called, it should:\n",
    " 1. Add variables named `filters` and `biases`, of appropriate shapes, to the graph.\n",
    " 2. Compute the 2-D discrete convolution of `filter` over `input_` using `tf.nn.conv2d`, using the correct filter size and strides, and add in the bias.\n",
    " 3. If `do_activation`, apply ReLU activation using `tf.nn.relu`.\n",
    " 4. If `add_summary`, then create a new 1-channel `tf.summary.image()`s for each channel of the activation (pre-pooling). This will allow us to visualize the activation maps of various filters throughout training. Note that each image needs a channel dimension, though it should be 1 here.\n",
    " 5. If `pool_size > 1`, uses `tf.nn.max_pool` to perform max pooling on the width and height axes.\n",
    " 6. Return the activations if `do_activation`, or the pre-activation otherwise.\n",
    " \n",
    "Hints:\n",
    " - Read [the tf.nn.conv2d documentation](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d) extensively! It tells you what shapes your variables need to have.\n",
    " - The layer will have one kernel and one bias per input channel\n",
    " - The `conv2d` strides should always be 1 in the batch and channel axes\n",
    " - In `max_pool`, the arguments `ksize` and `strides` will be the same\n",
    " - `padding` can be either 'SAME' or 'VALID'. I used 'SAME'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_conv_layer(input_, input_channels, n_filters,\n",
    "                    filter_size=3, stride=1, \n",
    "                    do_activation=True, pool_size=1,\n",
    "                    add_summary=False, postfix=''):\n",
    "    '''\n",
    "    Adds a convolutional layer to the model graph.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_: tensor\n",
    "        The activations of the previous layer, or \n",
    "        the input for the first dense layer.\n",
    "    input_channels: int\n",
    "        Number of channels in the input representation.\n",
    "    n_filters: int\n",
    "        Number of channels in the output representation.\n",
    "        Equivalently, number of filters in this layer.\n",
    "    filter_size: int\n",
    "        Width and height of each kernel in the layer's filters.\n",
    "    stride: int\n",
    "        Stride to use in the x and y directions for the\n",
    "        convolution operation.\n",
    "    do_activation: bool\n",
    "        Whether or not to apply ReLU activation.\n",
    "    pool_size: int\n",
    "        If > 1, does max pooling of this size to the\n",
    "        width and height axes of the activation.\n",
    "    add_summary: bool\n",
    "        Whether or not to log activations as summary images.\n",
    "    postfix: string\n",
    "        Postfix on name and variable scopes in this layer.\n",
    "        Used to simplify visualizations.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A tensor representing the activations of this layer.\n",
    "    '''\n",
    "    with tf.name_scope('conv' + postfix):\n",
    "        with tf.variable_scope('conv' + postfix):\n",
    "            pass # Define variables here\n",
    "        pass # Define operations here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4: Make the feature-extraction layers\n",
    "Now for the fun part.\n",
    "Use `make_conv_layer()` and `make_dense_layer()` to make the main part of the model.\n",
    "Set `add_summary=True` for at least one convolutional layer.\n",
    "\n",
    "Hints:\n",
    " - Try some layers with convolution and max pooling, then flatten (using `tf.reshape`) and add dense layers\n",
    " - The first convolutional layer has 1 input channel\n",
    " - The input is 64x64, so do lots of downsampling with strides and max pooling before you switch to dense layers to prevent having a huge number of parameters. (If you don't downsample at all, use 32 filters in the last convolutional layer, and use 128 units in the first dense layer, there will be 64\\*64\\*32\\*128 = ~17 million parameters in that dense layer alone! The whole model should ideally have less than 1 million parameters.)\n",
    " - If you're having trouble designing a model, try doing it in Keras first and visualizing the shapes and parameters with `model.summary()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5: Compute logits\n",
    "Use `make_dense_layer()` to make a final dense layer with `dim_output=10` and no activation to compute the final per-class logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6: Compute class probability for output\n",
    "Use `tf.nn.softmax` to compute the class probabilities.\n",
    "We will not use this for the loss, just for the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7: Compute cross-entropy loss\n",
    "Use `tf.nn.softmax_cross_entropy_with_logits_v2()` to compute the per-example loss, then `tf.reduce_mean()` to compute the mean loss for the batch.\n",
    "\n",
    "Add a summary scalar to plot loss in TensorBoard, and assign it to a variable since this time we'll need it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8: Optimizer and gradients\n",
    "Make an optimizer (I used `tf.train.MomentumOptimizer` with `lr=1e-3` and `momentum=0.9`) and an operation to apply the gradients (either `optimizer.minimize()`, or compute the gradients manually)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9: Predicted digit and per-batch accuracy\n",
    "The model should predict the digit it assigns the highest probability.\n",
    "Then, add a tensor which represents what fraction of the batch the model predicted correctly (its accuracy, or average 0/1 loss), and a summary operation for accuracy.\n",
    "\n",
    "Finally, add a tensor to merge the summaries made so far.\n",
    "\n",
    "Hint: to get the numerical value from the one-hot encoded label, use `tf.argmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: train the model\n",
    "Same training loop as always, with one small modification: you don't want to save every summary every batch because the `tf.summary.image()` operations save images.\n",
    "Run the loss summary operation every batch.\n",
    "\n",
    "\n",
    "Only run the full `tf.summary.merge_all()` operation once per every few batches, and when you do, use `feed_dict` to overwrite your input tensors with a \"batch\" of a single example (the same one every time).\n",
    "This will let you visualize in the TensorBoard Images tab how the activation maps of various filters on that one example change as the network trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: visualization\n",
    "Run TensorBoard, go to the Images tab, and look at how the final activation maps in various layers differ from each other, and (by dragging the slide bar at the top of each) how the activation map of a single filter develops as the network trains.\n",
    "\n",
    "This is what some activation maps of my first-layer, size-5 convolutions look like:\n",
    "![First-layer activations](./images/conv_outs.png)\n",
    "\n",
    "I see some interesting results here.\n",
    "The first and fourth filters seem to be activating on the background, the second detects the outside edges of the hand, and the third activates for sharp vertical gradients.\n",
    "\n",
    "Second-layer activation maps are a little more abstract, but still mostly make sense:\n",
    "![Second-layer activations](./images/conv_outs_2.png)\n",
    "The first one is really interesting, it seems to detect areas of high complexity in the image.\n",
    "\n",
    "My fourth-layer activation maps are too abstract to make any sense of:\n",
    "![Fourth-layer activations](./images/conv_outs_3.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
