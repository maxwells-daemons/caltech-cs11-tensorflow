{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Sentiment analysis with an LSTM network\n",
    "This week, we'll build a model for sentiment analysis, the problem of taking a string of text and predicting how positive an opinion it expresses.\"\n",
    "To do this, we'll use the last two big ideas in the course: vector embeddings and recurrent neural networks (with LSTM cells), trained on a dataset of [IMDB movie reviews](http://ai.stanford.edu/~amaas/data/sentiment/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Preprocess and understand the data\n",
    "This dataset is built into Keras, so it's very easy to import.\n",
    "I've written the preprocessing pipeline, but make sure to read it -- it'll be essential for understanding the data you're building a model for. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1: Load the data\n",
    "There are two hyperparameters here:\n",
    " - `maxlen`: The maximum number of words per review. Reviews longer than this are truncated. Keeping this low makes training faster by reducing the number of steps needed per example, but in practice we'd probably increase it. \n",
    " - `num_words`: The number of distinct words the dataset will contain. The `num_words` most common words are assigned unique tokens, and the rest are grouped into a single token.\n",
    " \n",
    "If training is taking forever, feel free to reduce `maxlen`.\n",
    "You can also try changing `num_words` to investigate the tradeoff it induces between the statistical and computational efficiency of having fewer unique words by grouping uncommon words and the advantages of recognizing more words.\n",
    "\n",
    "Reviews are returned as a sequence of integer tokens, each of which represents a distinct word.\n",
    "There are 3 special tokens:\n",
    " - 0 is a padding token (see below)\n",
    " - 1 is a token that represents the start of a review\n",
    " - 2 is a token that represents a word not in the model's vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "# Hyperparameters\n",
    "maxlen = 256\n",
    "num_words = 5000\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = \\\n",
    "    imdb.load_data(maxlen=maxlen, num_words=num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2: Pad all reviews to the same length\n",
    "Training is much more efficient when we can stack an entire batch of reviews together in a single tensor, so Keras requires that every training sequence is of the same length.\n",
    "To do this, we add padding tokens (the 0 token) to the beginning of every sequence to make them all of length `maxlen`.\n",
    "\n",
    "We pad the beginning of the sequence instead because padding the end would cause many steps of the RNN after it's read the last word in the review, causing the hidden state to lose information.\n",
    "In the model, we'll also tell the recurrent layers to mask out 0 values, so that the hidden state of the network is the same every time it reaches the start token (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3: Build word-token dictionaries\n",
    "In order to use the model with text outside of the dataset, we need to be able to convert words into tokens.\n",
    "We build two dictionaries:\n",
    " - `word_index` maps words into tokens\n",
    " - `index_word` maps tokens into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "index_word = {k + 3: v for (v, k) in word_index.items()}\n",
    "index_word[0] = '<PAD>'   # Special padding token\n",
    "index_word[1] = '<START>' # Special \"start of review\" token\n",
    "index_word[2] = '<OOV>'   # Special \"out of vocabulary\" token \n",
    "word_index = {k: v for (v, k) in index_word.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4: Using the dataset\n",
    "Below we print some summary statistics of the dataset and show how to convert between text and tokenized form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary statistics\n",
    "print(\n",
    "'''\n",
    "Training set size: {:}\n",
    "Test set size: {:}\n",
    "Numb|er of tokens: {:}\n",
    "Vocabulary size: {:}\n",
    "Proportion of words that are out-of-vocabulary: {:.4f}%\\n\n",
    "'''.format(x_train.shape, \n",
    "           x_test.shape, \n",
    "           len(index_word.keys()),\n",
    "           num_words,\n",
    "           np.mean(x_train == 1) * 100)\n",
    ")\n",
    "\n",
    "review_idx = 1\n",
    "review_tokens = x_train[review_idx]\n",
    "review_words = [index_word[idx] for idx in review_tokens]\n",
    "print('Review converted from tokens:\\n', ' '.join(review_words))\n",
    "print('\\nReview sentiment:', y_train[review_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Build a model\n",
    "The data and task really inform how we'll build the model here:\n",
    " - The input is variable-length sequences, so the feature extraction will be recurrent.\n",
    " - Each element of the input sequence is a word token, so the input is sparse and categorical. We'll deal with this by first computing embeddings.\n",
    " - The output is binary classification, so our model should produce a single probability independent of the length of the input sequence.\n",
    " \n",
    "Since this model has a lot of components, including recurrent layers, we'll stick to building the model completely in Keras.\n",
    "I used the functional API but the sequential API would also work here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Input and embedding layers\n",
    "Make an embedding layer that takes input of the correct shape and yields word embeddings.\n",
    "\n",
    "Notes:\n",
    " - `mask_zero` should be set to True, which will mask off the padding tokens we added before.\n",
    " - I used 64-dimensional embeddings.\n",
    " - Each input in a batch is a sequence of scalars (integer tokens) of length `maxlen`.\n",
    " - If you want to pass variable-length sequences as input, use None as the dimension on the sequence length axis of the input and don't specify an `input_length` for the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Recurrent feature-extraction layer\n",
    "Make an LSTM layer to summarize the variable-length sequence of embedding vectors into a fixed-size feature vector.\n",
    "\n",
    "Notes:\n",
    " - We're only interested in the last output of the LSTM layer.\n",
    " - I used 64 units.\n",
    " - You can add more layers if you like to make a deep LSTM network. If you do, the earlier layers should use `return_sequences` to yield an entire sequence of output vectors instead of just the last output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Output layer\n",
    "Add a dense layer to perform the final classification from the summary vector output by the LSTM layer to the probability that the input sequence expresses positive sentiment.\n",
    "\n",
    "Note that this is binary classification, so choose the layer's output size and activation function appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4: Compile and train model\n",
    "Compile and train the model.\n",
    "\n",
    "Notes:\n",
    " - RMSProp is usually a good choice for optimizing RNNs.\n",
    " - I used `clipnorm=1` in my optimizer to prevent exploding gradients.\n",
    " - I got about 90% accuracy after a couple of training epochs.\n",
    " - RNN training can take a while. Try training for a small number of epochs, or reducing `maxlen` if it takes too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Evaluate the model\n",
    "Below, I've pasted a review from IMDB and tokenized it.\n",
    "Add code to run your model over the review to predict whether it expresses positive or negative sentiment.\n",
    "\n",
    "Hint: Your model should output a single probability here, but expects a batch.\n",
    "You might need to use `np.expand_dims()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \\\n",
    "'''\n",
    "Pulp Fiction may be the single best film ever made, and quite appropriately\n",
    " it is by one of the most creative directors of all time, Quentin Tarantino.\n",
    " This movie is amazing from the beginning definition of pulp to the end \n",
    " credits and boasts one of the best casts ever assembled with the likes of\n",
    " Bruce Willis, Samuel L. Jackson, John Travolta, Uma Thurman, Harvey Keitel,\n",
    " Tim Roth and Christopher Walken. The dialog is surprisingly humorous for\n",
    " this type of film, and I think that\\'s what has made it so successful.\n",
    " Wrongfully denied the many Oscars it was nominated for, Pulp Fiction is by\n",
    " far the best film of the 90s and no Tarantino film has surpassed the \n",
    " quality of this movie (although Kill Bill came close). As far as I\\'m \n",
    " concerned this is the top film of all-time and definitely deserves a \n",
    " watch if you haven\\'t seen it.\n",
    "'''\n",
    "review = ''.join(list(filter(lambda x: x not in '\\',.()\\n', review.lower())))\n",
    "\n",
    "review_tokens = [1] # Begin with the <START> token\n",
    "for word in review.split():\n",
    "    review_tokens.append(word_index[word] if word in word_index.keys() else 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
